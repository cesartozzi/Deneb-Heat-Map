{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b54855-80f8-4b77-a042-d05fcf49f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import altair as alt\n",
    "import IPython.display as display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to show INFO-level messages\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d958db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BASE_URL = \"https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SIM/\"\n",
    "RAW_DIR = \"./sim_data/\"\n",
    "PROCESSED_DIR = \"./processed_data/\"\n",
    "PARQUET_DIR = \"./raw_parquet/\"\n",
    "YEARS = range(2015, 2024)  # Adjust year as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Download Files with Dual URL Format Support ---\n",
    "def download_files():\n",
    "    \"\"\"\n",
    "    Download mortality CSV files for each year, handling dual URL formats.\n",
    "    Saves each file in RAW_DIR with the filename '<year>.csv'.\n",
    "    \"\"\"\n",
    "    for year in tqdm(YEARS, desc=\"Downloading files\"):\n",
    "        # Choose URL format based on the year\n",
    "        if year <= 2021:\n",
    "            url = f\"https://diaad.s3.sa-east-1.amazonaws.com/sim/Mortalidade_Geral_{year}.csv\"\n",
    "        else:\n",
    "            url = f\"https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SIM/DO{str(year)[-2:]}OPEN.csv\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            output_file = os.path.join(RAW_DIR, f\"{year}.csv\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "            logging.info(f\"Downloaded file for year {year} successfully.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to download file for {year} from {url}: {e}\")\n",
    "\n",
    "# Uncomment the following line to run the download step:\n",
    "# download_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25948faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Convert CSV to Parquet ---\n",
    "\n",
    "def convert_all_csv_to_parquet():\n",
    "    \"\"\"\n",
    "    Convert all CSV files in RAW_DIR to Parquet format.\n",
    "    The output files are saved in PARQUET_DIR with the same base filename.\n",
    "    \"\"\"\n",
    "    csv_files = [f for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")]\n",
    "    \n",
    "    for filename in tqdm(csv_files, desc=\"Converting CSVs to Parquet\"):\n",
    "        year = filename.split(\".\")[0]  # Extract year from filename\n",
    "        input_path = os.path.join(RAW_DIR, filename)\n",
    "        output_path = os.path.join(PARQUET_DIR, f\"{year}.parquet\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV with proper encoding and data type preservation\n",
    "            df = pd.read_csv(\n",
    "                input_path,\n",
    "                sep=\";\",\n",
    "                encoding=\"latin-1\",\n",
    "                low_memory=False,\n",
    "                dtype={\n",
    "                    \"DTOBITO\": \"string\",\n",
    "                    \"CAUSABAS\": \"string\",\n",
    "                    \"CODMUNRES\": \"string\"\n",
    "                }\n",
    "            )\n",
    "            # Save as Parquet with snappy compression\n",
    "            df.to_parquet(\n",
    "                output_path,\n",
    "                engine=\"pyarrow\",\n",
    "                compression=\"snappy\"\n",
    "            )\n",
    "            logging.info(f\"Converted {filename} to Parquet successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error converting {filename}: {e}\")\n",
    "\n",
    "# Uncomment the following line to convert all CSV files:\n",
    "# convert_all_csv_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d52742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(file_name):\n",
    "    \"\"\"\n",
    "    Convert a specific CSV file to Parquet format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): Name of the CSV file in RAW_DIR to convert.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(RAW_DIR, file_name)\n",
    "    output_path = os.path.join(PARQUET_DIR, f\"{file_name.split('.')[0]}.parquet\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            input_path,\n",
    "            sep=\";\",\n",
    "            encoding=\"latin-1\",\n",
    "            low_memory=False,\n",
    "            dtype={\n",
    "                \"DTOBITO\": \"string\",\n",
    "                \"CAUSABAS\": \"string\",\n",
    "                \"CODMUNRES\": \"string\"\n",
    "            }\n",
    "        )\n",
    "        df.to_parquet(\n",
    "            output_path,\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\"\n",
    "        )\n",
    "        logging.info(f\"Successfully converted {file_name} to Parquet.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting {file_name}: {e}\")\n",
    "\n",
    "# Example usage for a specific file:\n",
    "# convert_csv_to_parquet(\"2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Aggregate Mortality Data ---\n",
    "def aggregate_obitos():\n",
    "    \"\"\"\n",
    "    Aggregate mortality data from Parquet files.\n",
    "    \n",
    "    Reads each Parquet file, processes the 'DTOBITO' date field,\n",
    "    groups data by date, and adds columns for year and day of year.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the aggregated daily counts.\n",
    "    \"\"\"\n",
    "    summary_list = []\n",
    "    \n",
    "    for file in os.listdir(PARQUET_DIR):\n",
    "        if file.endswith(\".parquet\"):\n",
    "            year_str = file.split('.')[0]\n",
    "            file_path = os.path.join(PARQUET_DIR, file)\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "                \n",
    "                # Process the 'DTOBITO' column: pad with zeros and convert to datetime\n",
    "                df[\"DTOBITO\"] = df[\"DTOBITO\"].astype(str).str.zfill(8)\n",
    "                df[\"DTOBITO\"] = pd.to_datetime(df[\"DTOBITO\"], format=\"%d%m%Y\", errors=\"coerce\")\n",
    "                \n",
    "                # Normalize date (remove time component)\n",
    "                df[\"date\"] = df[\"DTOBITO\"].dt.normalize()\n",
    "                \n",
    "                # Group by date and count the number of records (obitos)\n",
    "                daily_counts = df.groupby(\"date\").size().reset_index(name=\"number_of_obitos\")\n",
    "                \n",
    "                # Add the year column as an integer and day of year column\n",
    "                daily_counts[\"year\"] = int(year_str)\n",
    "                daily_counts[\"day_number\"] = daily_counts[\"date\"].dt.dayofyear\n",
    "                \n",
    "                summary_list.append(daily_counts)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    if summary_list:\n",
    "        final_summary = pd.concat(summary_list, ignore_index=True)\n",
    "        final_summary.sort_values(by=[\"year\", \"date\"], inplace=True)\n",
    "        return final_summary\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a444ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data and save it to CSV\n",
    "result_df = aggregate_obitos()\n",
    "if not result_df.empty:\n",
    "    output_csv = os.path.join(PROCESSED_DIR, \"combined_data.csv\")\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    logging.info(f\"Aggregated data saved to {output_csv}.\")\n",
    "else:\n",
    "    logging.warning(\"No data was aggregated; the resulting DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Visualization with Altair ---\n",
    "alt.renderers.enable('default')\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "chart = alt.Chart(result_df).mark_rect().encode(\n",
    "    x=alt.X('day_number:Q', title='Day of Year'),\n",
    "    y=alt.Y('year:N', title='Year'),\n",
    "    color=alt.Color('number_of_obitos:Q', title='Number of Obitos', scale=alt.Scale(scheme='reds')),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('year:N', title='Year'),\n",
    "        alt.Tooltip('day_number:Q', title='Day of Year'),\n",
    "        alt.Tooltip('number_of_obitos:Q', title='Number of Obitos')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title=\"Daily Obitos Heatmap\"\n",
    ")\n",
    "\n",
    "display.display(chart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
